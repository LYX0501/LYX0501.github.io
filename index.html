<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuxing Long | 龙宇星</title>

    <meta name="author" content="Yuxing Long">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>✨</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuxing Long | 龙宇星
                </p>
                <p>I am a first-year PhD candidate research visitor in <a href="https://cfcs.pku.edu.cn/english/"> Center on Frontiers of Computing Studies (CFCS)</a> at <a href="https://english.pku.edu.cn/">Peking University</a>, advised by Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a>. Before this, I obtained my Bachelor's and Master's degrees from Beijing University of Posts and Telecommunications (BUPT).
                </p>
                <p>My research interests include embodied navigation, multimodal pretraining, and multimodal conversation.
                </p>
                <p>
                 Email: longyuxing [at] bupt.edu.cn
                </p>
                <p style="text-align:center">
                  <a href="longyuxing@bupt.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://github.com/LYX0501/">Github</a> &nbsp;/&nbsp;
		  <a href="https://scholar.google.com/citations?user=UqQ41BIAAAAJ&hl=zh-CN/">Google Scholar</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%;text-align:center;">
                <a href="images/photo.jpg"><img style="width:70%;max-width:70%" alt="profile photo" src="images/photo.jpg" class="hoverZoomLink"></a>
                <br>
                <table style="width:100%;"><tr><td></td></tr></table>
                <a href="https://badges.toozhao.com/stats/01HB87MKS7YZ7860H8H7V626Q1"><img src="https://badges.toozhao.com/badges/01HB87MKS7YZ7860H8H7V626Q1/orange.svg" /></a>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr onmouseout="discussnav_stop()" onmouseover="instructnav_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='InstructNav_image'>
            <img src='images/2024InstructNav.jpg' width="170"></div>
            <img src='images/2024InstructNav.jpg' width="170">
          </div>
          <script type="text/javascript">
            function instructnav_start() {
              document.getElementById('InstructNav_image').style.opacity = "1";
            }

            function instructnav_stop() {
              document.getElementById('InstructNav_image').style.opacity = "0";
            }
            instructnav_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2406.04882.pdf">
            <span class="papertitle">InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Wenzhe Cai, Hongcheng Wang, Guanqi Zhan, Hao Dong
          <br>
          <em>Arxiv</em>
          <br>
          <a href="https://arxiv.org/pdf/2406.04882.pdf">Paper</a>
	  /
	  <a href="https://sites.google.com/view/instructnav">Project</a>
	  /
	  Code
          <p></p>
          <p>
          The first zero-shot generic instruction navigation system without any pre-built maps.
          </p>
        </td>
      </tr>
    
      <tr onmouseout="discussnav_stop()" onmouseover="discussnav_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='DiscussNav_image'>
            <img src='images/2023DiscussNav.jpg' width="170"></div>
            <img src='images/2023DiscussNav.jpg' width="170">
          </div>
          <script type="text/javascript">
            function discussnav_start() {
              document.getElementById('DiscussNav_image').style.opacity = "1";
            }

            function discussnav_stop() {
              document.getElementById('DiscussNav_image').style.opacity = "0";
            }
            discussnav_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2309.11382.pdf">
            <span class="papertitle">Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Xiaoqi Li, Wenzhe Cai, Hao Dong
          <br>
          <em>International Conference on Robotics and Automation (ICRA) 2024</em>
          <br>
          <a href="https://arxiv.org/pdf/2309.11382.pdf">Paper</a>
	  /
	  <a href="https://sites.google.com/view/discussnav">Project</a>
	  /
	  <a href="https://github.com/LYX0501/DiscussNav">Code</a>
          <p></p>
          <p>
          DiscussNav agent actively discusses with multiple domain experts before moving.
          </p>
        </td>
      </tr>

      <tr onmouseout="dstc11_stop()" onmouseover="dstc11_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='DSTC11_image'>
            <img src='images/2023DSTC11.jpg' width="170"></div>
            <img src='images/2023DSTC11.jpg' width="170">
          </div>
          <script type="text/javascript">
            function dstc11_start() {
              document.getElementById('DSTC11_image').style.opacity = "1";
            }

            function dstc11_stop() {
              document.getElementById('DSTC11_image').style.opacity = "0";
            }
            dstc11_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://github.com/LYX0501/DAMO-ConvAI/tree/main/dstc11-simmc">
            <span class="papertitle">Improving Situated Conversational Agents with Step-by-Step Multi-modal Logic Reasoning</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>*, Huibin Zhang*, Binyuan Hui*, Zhenglu Yang, Caixia Yuan, Xiaojie Wang, Fei Huang, Yongbin Li
          <br>
          <em><b style="color: red;">Champion of SIMMC 2.1 Competition</b></em>, 
          <em>DSTC 11 Workshop</em>
          <b style="color: red;">(Best Paper)</b>
          <br>
	  <a href="https://aclanthology.org/2023.dstc-1.3.pdf">Paper</a>
          /
          <a href="https://github.com/LYX0501/DAMO-ConvAI/tree/main/dstc11-simmc">Code</a>
          <p></p>
          <p>
          We propose a dual-system framework to conduct multimodal logic reasoning step-by-step.
          </p>
        </td>
      </tr>
            
      <tr onmouseout="reg_stop()" onmouseover="reg_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='REG_image'>
            <img src='images/2023MM_REG.png' width="170"></div>
            <img src='images/2023MM_REG.png' width="170">
          </div>
          <script type="text/javascript">
            function reg_start() {
              document.getElementById('REG_image').style.opacity = "1";
            }

            function reg_stop() {
              document.getElementById('REG_image').style.opacity = "0";
            }
            reg_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2308.09977v1.pdf">
            <span class="papertitle">Whether you can locate or not? Interactive Referring Expression Generation</span>
          </a>
          <br>
          Fulong Ye, <strong>Yuxing Long</strong>, Fangxiang Feng, Xiaojie Wang
          <br>
          <em>ACM Multimedia (MM) 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2308.09977v1.pdf">Paper</a>
          /
          <a href="https://github.com/superhero-7/ireg">Code</a>
          <p></p>
          <p>
          We generate referring expressions by multi-round communications.
          </p>
        </td>
      </tr>
            
      <tr onmouseout="sure_stop()" onmouseover="sure_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='SURE_image'>
            <img src='images/2023ACL_SURE.jpg' width="170"></div>
            <img src='images/2023ACL_SURE.jpg' width="170">
          </div>
          <script type="text/javascript">
            function sure_start() {
              document.getElementById('SURE_image').style.opacity = "1";
            }

            function sure_stop() {
              document.getElementById('SURE_image').style.opacity = "0";
            }
            sure_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2305.18212.pdf">
            <span class="papertitle">Multimodal Recommendation Dialog with Subjective Preference: A New Challenge and Benchmark</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Binyuan Hui, Caixia Yuan, Fei Huang, Yongbin Li, Xiaojie Wang
          <br>
          <em>Findings of the Association for Computational Linguistics (Findings of ACL) 2023</em>
          <br>
          <a href="https://arxiv.org/pdf/2305.18212.pdf">Paper</a>
          <p></p>
          <p>
          A new dataset for multimodal recommendation dialog with subjective preferences.
          </p>
        </td>
      </tr>

      <tr onmouseout="spring_stop()" onmouseover="spring_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='SPRING_image'>
            <img src='images/2023AAAI_SPRING.png' width="170"></div>
            <img src='images/2023AAAI_SPRING.png' width="170">
          </div>
          <script type="text/javascript">
            function spring_start() {
              document.getElementById('SPRING_image').style.opacity = "1";
            }

            function spring_stop() {
              document.getElementById('SPRING_image').style.opacity = "0";
            }
            spring_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2301.01949.pdf">
            <span class="papertitle">SPRING: Situated Conversation Agent Pretrained with Multimodal Questions from Incremental Layout Graph</span>
          </a>
          <br>
          <strong>Yuxing Long</strong>, Binyuan Hui, Fulong Ye, Yanyang Li, Zhuoxin Han, Caixia Yuan, Yongbin Li, Xiaojie Wang
          <br>
          <em>AAAI Conference on Artificial Intelligence 2023</em>
          <b style="color: red;">(Oral presentation)</b>
          <br>
          <a href="https://arxiv.org/pdf/2301.01949.pdf">Paper</a>
          /
          <a href="https://github.com/LYX0501/SPRING">Code</a>
          <p></p>
          <p>
          We improve the situated conversation agent through novel multimodal question-answering pretraining tasks.
          </p>
        </td>
      </tr>
            
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Services</h2>
                <br> 
                <p>
                  Reviewer: ACM MM <em>2023</em>
		  <br>
		  Reviewer: NeurIPS <em>2023</em> <a href="https://sslneurips23.github.io/">Self-Supervised Learning - Theory and Practice Workshop</a>
		  <br>
		  Reviewer: NeurIPS <em>2022</em> <a href="https://neurips-hill.github.io/">Human in the Loop Learning (HiLL) Workshop</a>
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Awards and Honors</h2>
                <br> 
                <p>
                  National Scholarship, 2023 <br>
		  Excellent Graduate, Beijing University of Posts and Telecommunications, 2023 <br>
                </p>
              </td>
            </tr>
          </tbody></table>
		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
